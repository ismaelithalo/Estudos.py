{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ismaelithalo/Estudos.py/blob/master/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUggRubLaBr"
      },
      "source": [
        "## Q-Learning com Gym\n",
        "\n",
        "Adaptado do [tutorial de S. Kansal e B. Martin](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)\n",
        "\n",
        "Abrindo e mostrando um ambiente pronto do gym que simula o problema do Taxi (versão 2):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOIZcsERLZSP",
        "outputId": "89acd49c-0f81-4889-d217-9faf22b3e30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mya2l0h9YcVn"
      },
      "source": [
        "### Agente:\n",
        "\n",
        "* Retângulo amarelo ou verde (fica verde quando está levando um passageiro)\n",
        "\n",
        "### Ambiente:\n",
        "\n",
        "- 16 possíveis posições possíveis\n",
        "- 4 posições fixas para pegar (saída) ou largar (chegada) o passageiro (R,G,Y,B)\n",
        "- Cor azul indica local de saída do passageiro e cor rosa o local de chegada\n",
        "- Alguns muros indicados pelo símbolo \"|\"\n",
        "\n",
        "### Estados:\n",
        "\n",
        "500 possíveis combinações que o ambiente pode assumir\n",
        "\n",
        "### Ações\n",
        "\n",
        "* 0 = sul (mover taxi para baixo)\n",
        "* 1 = norte (mover taxi para cima)\n",
        "* 2 = leste (mover taxi para direita)\n",
        "* 3 = oeste (mover taxi para esquerda)\n",
        "* 4 = pegar o passageiro\n",
        "* 5 = largar o passageiro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhK_phsuXxox",
        "outputId": "e48af008-acb5-46fe-c495-889b1a14a841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Total de Ações {}\".format(env.action_space))\n",
        "print(\"Total de Estados {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Ações Discrete(6)\n",
            "Total de Estados Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNDUC5XNa1AK"
      },
      "source": [
        "### Alterando o estado\n",
        "\n",
        "Use a função env.encode(taxi_linha,taxi_coluna,passageiro_saida,passageiro_chegada), o local de saída é um valor de 0 a 3 indicado cada uma das 4 possíveis posições de saída e chegada: R (0), G (1), Y (2) ou B (3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdAlu52DXyDw",
        "outputId": "3415690d-6acf-448f-e240-838f884bffa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "state = env.encode(3, 1, 2, 1)\n",
        "print(\"Número do estado:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()\n",
        "\n",
        "env.s = 369\n",
        "print(\"Número do estado:\", env.s)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número do estado: 329\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Número do estado: 369\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpV02LMuc5CT"
      },
      "source": [
        "### Função de reforço\n",
        "\n",
        "Valor de reforço para cada combinação de estado X ação (ou seja, uma tabela com 500 * 6 = 3000 posições no caso deste problema)\n",
        "\n",
        "[(sempre_um, próximo_estado, valor_do_reforço, atingiu_o_objetivo)]\n",
        "\n",
        "Possíveis punições:\n",
        "* -1 : Cada movimento feito pelo carro ou tentativa de bater no muro\n",
        "* -10 : Pegar ou largar o passageiro no lugar errado\n",
        "\n",
        "Possíveis recompensas:\n",
        "* +20 : Deixar o passageiro no lugar certo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akeOkzw2Xy1x",
        "outputId": "1f9811d3-52b0-4ae1-e944-df5c61bd5863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "env.P[329]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 429, -1, False)],\n",
              " 1: [(1.0, 229, -1, False)],\n",
              " 2: [(1.0, 349, -1, False)],\n",
              " 3: [(1.0, 329, -1, False)],\n",
              " 4: [(1.0, 329, -10, False)],\n",
              " 5: [(1.0, 329, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSFd4dC_eJvq"
      },
      "source": [
        "### Resolvendo usando Ações Aleatórias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSeXUtwveKEa",
        "outputId": "2232694b-2068-4a5f-ef2a-7cc1dbfcd894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env.s = 329  # começa no estado do exemplo acima\n",
        "\n",
        "epochs = 0   # total de ações realizadas\n",
        "penalties = 0   # quantidade de punições recebidas por pegar ou largar no lugar errado\n",
        "\n",
        "frames = [] # usado para fazer uma animação\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # escolhe aleatoriamente uma ação\n",
        "    state, reward, done, info = env.step(action)  # aplica a ação e pega o resultado\n",
        "\n",
        "    if reward == -10:  # conta uma punição\n",
        "        penalties += 1\n",
        "\n",
        "    # Quarda a sequência para poder fazer a animação depois\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "\n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 300\n",
            "Total de penalizações recebidas: 72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn3osqjIeKjL"
      },
      "source": [
        "### Mostrando a animação dos movimentos realizados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8uiG5MBgSXv",
        "outputId": "d64b9da5-5024-4406-c870-6ef511b12178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "\n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 300\n",
            "State: 85\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5kNfr9efDvZ"
      },
      "source": [
        "### Aprendendo a resolver usando Q-Learning\n",
        "\n",
        "Tenho uma vídeo explicando o Q-Learning aqui: https://youtu.be/zQUFxZsZODY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txSfEmTLeKtK",
        "outputId": "de33f8e2-4284-43e0-dae1-2101c3767f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inicialização com a tabela de valores Q\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hiperparâmetros\n",
        "alpha = 0.1   # taxa de aprendizagem\n",
        "gamma = 0.6   # fator de desconto\n",
        "epsilon = 0.1  # chance de escolha aleatória\n",
        "\n",
        "# Total geral de ações executadas e penalidades recebidas durante a aprendizagem\n",
        "epochs, penalties = 0,0\n",
        "\n",
        "for i in range(1, 100001): # Vai rodar 100000 diferentes versões do problema\n",
        "    state = env.reset()  # Inicialização aleatoria do ambient\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Escolhe ação aleatoriamente\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Escolhe ação com base no que já aprendeu\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) # Aplica a ação\n",
        "\n",
        "        old_value = q_table[state, action]  # Valor da ação escolhida no estado atual\n",
        "        next_max = np.max(q_table[next_state]) # Melhor valor no próximo estado\n",
        "\n",
        "        # Atualize o valor Q usando a fórmula principal do Q-Learning\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:  # Contabiliza as punições por pegar ou deixar no lugar errado\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state # Muda de estado\n",
        "        epochs += 1\n",
        "\n",
        "\n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 1523452\n",
            "Total de penalizações recebidas: 46360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWMOzsS0fEZb"
      },
      "source": [
        "### Mostrando a tabela Q para o estado 329"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cBAaoqGg1y_",
        "outputId": "a182c8da-1065-4222-dae4-5f2bb56617d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "env.s = 329\n",
        "env.render()\n",
        "\n",
        "q_table[329]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.48874372,  -2.47061344,  -2.48848242,  -2.48086628,\n",
              "       -10.9198647 , -11.11583968])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2L-4fN2g9YO"
      },
      "source": [
        "### Resolvendo o problema com o aprendizado adquirido\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mguXdCZ_rwoN",
        "outputId": "05e47466-d942-43d8-db85-e7eef332dde5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "state = 329\n",
        "epochs, penalties = 0, 0\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "     action = np.argmax(q_table[state])\n",
        "     state, reward, done, info = env.step(action)\n",
        "\n",
        "     if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "     epochs += 1\n",
        "\n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 14\n",
            "Total de penalizações recebidas: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "own561m4ryI5"
      },
      "source": [
        "### Calculando o desempenho médio para várias versões do problema\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIZZHe_Qg9p2",
        "outputId": "8c8cbddb-a4e6-480c-afa5-b8c463e285e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Resultados depois de {episodes} simulações:\")\n",
        "print(f\"Média de ações por simulação: {total_epochs / episodes}\")\n",
        "print(f\"Média de penalidades: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultados depois de 100 simulações:\n",
            "Média de ações por simulação: 12.37\n",
            "Média de penalidades: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}